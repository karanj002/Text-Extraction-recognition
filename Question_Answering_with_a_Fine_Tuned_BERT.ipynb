{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-1zl5XdYInf"
   },
   "source": [
    "# Question Answering with a Fine-Tuned BERT\n",
    "*by Chris McCormick*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_mAnIPKaXyw"
   },
   "source": [
    "What does it mean for BERT to achieve \"human-level performance on Question Answering\"? Is BERT the greatest search engine ever, able to find the answer to any question we pose it?\n",
    "\n",
    "In **Part 1** of this post / notebook, I'll explain what it really means to apply BERT to QA, and illustrate the details.\n",
    "\n",
    "**Part 2** contains example code--we'll be downloading a model that's *already been fine-tuned* for question answering, and try it out on our own text! \n",
    "\n",
    "For something like text classification, you definitely want to fine-tune BERT on your own dataset. For question answering, however, it seems like you may be able to get decent results using a model that's already been fine-tuned on the SQuAD benchmark. In this Notebook, we'll do exactly that, and see that it performs well on text that wasn't in the SQuAD dataset.\n",
    "\n",
    "**Links**\n",
    "\n",
    "* My [video walkthrough](https://youtu.be/l8ZYCvgGu0o) on this topic. \n",
    "* The blog post version.\n",
    "* The [Colab Notebook](https://colab.research.google.com/drive/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2bUvKUffHNY"
   },
   "source": [
    "# Part 1: How BERT is applied to Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su7fixBdiUex"
   },
   "source": [
    "## The SQuAD v1.1 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT5ESKDxfnLf"
   },
   "source": [
    "When someone mentions \"Question Answering\" as an application of BERT, what they are really referring to is applying BERT to the Stanford Question Answering Dataset (SQuAD).\n",
    "\n",
    "The task posed by the SQuAD benchmark is a little different than you might think. Given a question, and *a passage of text containing the answer*, BERT needs to highlight the \"span\" of text corresponding to the correct answer. \n",
    "\n",
    "The SQuAD homepage has a fantastic tool for exploring the questions and reference text for this dataset, and even shows the predictions made by top-performing models.\n",
    "\n",
    "For example, here are some [interesting examples](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/Super_Bowl_50.html?model=r-net+%20(ensemble)%20(Microsoft%20Research%20Asia)&version=1.1) on the topic of Super Bowl 50.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xN5f1bxf6K_"
   },
   "source": [
    "## BERT Input Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ctum5SK6f9uP"
   },
   "source": [
    "To feed a QA task into BERT, we pack both the question and the reference text into the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaPCFHgSOO37"
   },
   "source": [
    "\n",
    "[![Input format for QA](https://drive.google.com/uc?export=view&id=1dfgTaE_SABpr2blqwTjq9PTyhYabO8_m)](https://drive.google.com/uc?export=view&id=1dfgTaE_SABpr2blqwTjq9PTyhYabO8_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-SS0PucOQdt"
   },
   "source": [
    "\n",
    "The two pieces of text are separated by the special `[SEP]` token. \n",
    "\n",
    "BERT also uses \"Segment Embeddings\" to differentiate the question from the reference text. These are simply two embeddings (for segments \"A\" and \"B\") that BERT learned, and which it adds to the token embeddings before feeding them into the input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl6BECEXu4jH"
   },
   "source": [
    "[![BERT eBook Display Ad](https://drive.google.com/uc?export=view&id=1d6L584QYqpREpRIwAZ55Wsq8AUs5qSk1)](https://bit.ly/3isUyCt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs31dcrPg5Tg"
   },
   "source": [
    "## Start & End Token Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvOdUa9Wg-Uv"
   },
   "source": [
    "BERT needs to highlight a \"span\" of text containing the answer--this is represented as simply predicting which token marks the start of the answer, and which token marks the end.\n",
    "\n",
    "![Start token classification](http://www.mccormickml.com/assets/BERT/SQuAD/start_token_classification.png)\n",
    "\n",
    "For every token in the text, we feed its final embedding into the start token classifier. The start token classifier only has a single set of weights (represented by the blue \"start\" rectangle in the above illustration) which it applies to every word.\n",
    "\n",
    "After taking the dot product between the output embeddings and the 'start' weights, we apply the softmax activation to produce a probability distribution over all of the words. Whichever word has the highest probability of being the start token is the one that we pick.\n",
    "\n",
    "We repeat this process for the end token--we have a separate weight vector this.\n",
    "\n",
    "![End token classification](http://www.mccormickml.com/assets/BERT/SQuAD/end_token_classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "457VPa20fZzY"
   },
   "source": [
    "# Part 2: Example Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wpCLgVCkki5"
   },
   "source": [
    "In the example code below, we'll be downloading a model that's *already been fine-tuned* for question answering, and try it out on our own text.\n",
    "\n",
    "If you do want to fine-tune on your own dataset, it is possible to fine-tune BERT for question answering yourself. See [run_squad.py](https://github.com/huggingface/transformers/blob/master/examples/run_squad.py) in the `transformers` library. However,you may find that the below \"fine-tuned-on-squad\" model already does a good job, even if your text is from a different domain. \n",
    "\n",
    "> Note: The example code in this Notebook is a commented and expanded version of the short example provided in the `transformers` documentation [here](https://huggingface.co/transformers/model_doc/bert.html?highlight=bertforquestionanswering#transformers.BertForQuestionAnswering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVq-TuylYRDW"
   },
   "source": [
    "## 1. Install huggingface transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9nhy3PzGQ44"
   },
   "source": [
    "This example uses the `transformers` [library](https://github.com/huggingface/transformers/) by huggingface. We'll start by installing the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQl0MMrOGIup",
    "outputId": "eca77275-3395-4688-d3b3-fd09ea480f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 17.7 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.4/78.4 kB ? eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.2/86.2 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 23.2 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "     ---------------------------------------- 151.7/151.7 kB ? eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.6.2-cp310-cp310-win_amd64.whl (262 kB)\n",
      "     ------------------------------------- 262.0/262.0 kB 15.7 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "     ---------------------------------------- 139.0/139.0 kB ? eta 0:00:00\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "     ------------------------------------- 160.2/160.2 kB 10.0 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.2/61.2 kB ? eta 0:00:00\n",
      "Installing collected packages: tokenizers, urllib3, tqdm, regex, pyyaml, numpy, idna, filelock, charset-normalizer, certifi, requests, huggingface-hub, transformers\n",
      "Successfully installed certifi-2022.6.15 charset-normalizer-2.0.12 filelock-3.7.1 huggingface-hub-0.7.0 idna-3.3 numpy-1.22.4 pyyaml-6.0 regex-2022.6.2 requests-2.28.0 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.19.4 urllib3-1.26.9\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-ONLrgJK99TQ"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WThOUtpYvG-"
   },
   "source": [
    "## 2. Load Fine-Tuned BERT-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaweLnNXGhTY"
   },
   "source": [
    "For Question Answering we use the `BertForQuestionAnswering` class from the `transformers` library.\n",
    "\n",
    "This class supports fine-tuning, but for this example we will keep things simpler and load a BERT model that has already been fine-tuned for the SQuAD benchmark.\n",
    "\n",
    "The `transformers` library has a large collection of pre-trained models which you can reference by name and load easily. The full list is in their documentation [here](https://huggingface.co/transformers/pretrained_models.html).\n",
    "\n",
    "For Question Answering, they have a version of BERT-large that has already been fine-tuned for the SQuAD benchmark. \n",
    "\n",
    "BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance. \n",
    "\n",
    "(Note that this download is not using your own network bandwidth--it's between the Google instance and wherever the model is stored on the web).\n",
    "\n",
    "Note: I believe this model was trained on version 1 of SQuAD, since it's not outputting whether the question is \"impossible\" to answer from the text (which is part of the task in v2 of SQuAD).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "17108037891642fd8535f898ffc1c666",
      "454c54b5f7e34caabfcdfb3751647f1d",
      "2e1db44876aa4bda8f432424d43ef49d",
      "d2057bb0a03148d89ff752025b209034",
      "625f0bb8d61443ffa9aec2e2f52db015",
      "658e95de041f4b6abbd70a8071253297",
      "296fc3b5f35f49a4a8b71ba7450f39ba",
      "0f4a53ffb84641be966022b5f995d597",
      "0325dd797ca640cea44dc5d2d50d3378",
      "a5d823c6c36f4ffaa54de63b7df27187",
      "bdec4b4708b1446bbeb00ddef16511c7",
      "183df2bf188a4f9fba59234e57b205e4",
      "eb910005a7ae4e04ab28ac8d506e9f9c",
      "b871a4177dba4977813c16a6a71d3b27",
      "3f1e67a35637482e9a2db7e7233d02c2",
      "ba7cd4be6dcb457db7197cd326a56793"
     ]
    },
    "id": "-Mnv95sX-U9K",
    "outputId": "2744c594-ef24-437d-d158-956918c40a11"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5021dd1abcd844f1872fb727ee6f29ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1abf428ef34cb5a0558badf96e8203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8imoOxoqGZ0h"
   },
   "source": [
    "Load the tokenizer as well. \n",
    "\n",
    "Side note: Apparently the vocabulary of this model is identicaly to the one in bert-base-uncased. You can load the tokenizer from `bert-base-uncased` and that works just as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "604df6fab59c46438be9ac909632e6fa",
      "54decd45844c47ea8c2816ef729e7445",
      "9c50285033b941c19bb0d6cfefa35a92",
      "9cdafc71790e4a65afdea6267db7ea29",
      "7773da342fd34ddd8d98d22308ff84a0",
      "15cf214d3f95421790d246a7fdc8d200",
      "2c9c2fd7eece41589d73d9a238225905",
      "c7052a47457d44e2b4914734294eb671"
     ]
    },
    "id": "SFQ5f7gv-RBH",
    "outputId": "1d000603-9233-4321-9efb-6eb176ba228e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0b5174224340598c554411646d93b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90683b13e3a4ed8a5a9678e4f51095e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I__1ubvcZYow"
   },
   "source": [
    "## 3. Ask a Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8MQ7b-GJIcM"
   },
   "source": [
    "Now we're ready to feed in an example!\n",
    "\n",
    "A QA example consists of a question and a passage of text containing the answer to that question.\n",
    "\n",
    "Let's try an example using the text in this tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kWzZP4EN-Zxg"
   },
   "outputs": [],
   "source": [
    "question = \"What converts text to pdf file into images?\"\n",
    "answer_text = \"\"\"First we have sepcified the filr locations so as to edit in the future\n",
    "\n",
    "easyocr.Reader()- sets english as ur language\n",
    "\n",
    "convert_from_path converts the text in the pdf file into images\n",
    "\n",
    "bounds- using easyocr module it reads the text and then converts the imges into an array\n",
    "and setting the parameters like minimum size y and x height and width and these tell us the\n",
    "minimum threshold for using a decoder, here m using beamsearch.\n",
    "This builds a bounding box on main criterias\n",
    "\n",
    "\n",
    "en_core_web_sm = nowpassing it to spacy and loading the default english model in spacy as nlp\n",
    "\n",
    "the display cmd gives us the text from the pdf with marked criterias which are important and\n",
    "relevant like name, date, Geopolitical entity, and org\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYoX33CfKGsr",
    "outputId": "14b9e3fe-5afc-46d8-e6fa-25d183674aff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 178 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iow838yPNDTv",
    "outputId": "0ffb2a5f-fdae-43ca-b146-b1604cc8bab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "what          2,054\n",
      "converts     19,884\n",
      "text          3,793\n",
      "to            2,000\n",
      "pdf          11,135\n",
      "file          5,371\n",
      "into          2,046\n",
      "images        4,871\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "first         2,034\n",
      "we            2,057\n",
      "have          2,031\n",
      "sep          19,802\n",
      "##ci          6,895\n",
      "##fied       10,451\n",
      "the           1,996\n",
      "fi           10,882\n",
      "##lr         20,974\n",
      "locations     5,269\n",
      "so            2,061\n",
      "as            2,004\n",
      "to            2,000\n",
      "edit         10,086\n",
      "in            1,999\n",
      "the           1,996\n",
      "future        2,925\n",
      "easy          3,733\n",
      "##oc         10,085\n",
      "##r           2,099\n",
      ".             1,012\n",
      "reader        8,068\n",
      "(             1,006\n",
      ")             1,007\n",
      "-             1,011\n",
      "sets          4,520\n",
      "english       2,394\n",
      "as            2,004\n",
      "ur           24,471\n",
      "language      2,653\n",
      "convert      10,463\n",
      "_             1,035\n",
      "from          2,013\n",
      "_             1,035\n",
      "path          4,130\n",
      "converts     19,884\n",
      "the           1,996\n",
      "text          3,793\n",
      "in            1,999\n",
      "the           1,996\n",
      "pdf          11,135\n",
      "file          5,371\n",
      "into          2,046\n",
      "images        4,871\n",
      "bounds       19,202\n",
      "-             1,011\n",
      "using         2,478\n",
      "easy          3,733\n",
      "##oc         10,085\n",
      "##r           2,099\n",
      "module       11,336\n",
      "it            2,009\n",
      "reads         9,631\n",
      "the           1,996\n",
      "text          3,793\n",
      "and           1,998\n",
      "then          2,059\n",
      "converts     19,884\n",
      "the           1,996\n",
      "im           10,047\n",
      "##ges         8,449\n",
      "into          2,046\n",
      "an            2,019\n",
      "array         9,140\n",
      "and           1,998\n",
      "setting       4,292\n",
      "the           1,996\n",
      "parameters   11,709\n",
      "like          2,066\n",
      "minimum       6,263\n",
      "size          2,946\n",
      "y             1,061\n",
      "and           1,998\n",
      "x             1,060\n",
      "height        4,578\n",
      "and           1,998\n",
      "width         9,381\n",
      "and           1,998\n",
      "these         2,122\n",
      "tell          2,425\n",
      "us            2,149\n",
      "the           1,996\n",
      "minimum       6,263\n",
      "threshold    11,207\n",
      "for           2,005\n",
      "using         2,478\n",
      "a             1,037\n",
      "deco         21,933\n",
      "##der         4,063\n",
      ",             1,010\n",
      "here          2,182\n",
      "m             1,049\n",
      "using         2,478\n",
      "beams        13,110\n",
      "##ear        14,644\n",
      "##ch          2,818\n",
      ".             1,012\n",
      "this          2,023\n",
      "builds       16,473\n",
      "a             1,037\n",
      "bound         5,391\n",
      "##ing         2,075\n",
      "box           3,482\n",
      "on            2,006\n",
      "main          2,364\n",
      "criteria      9,181\n",
      "##s           2,015\n",
      "en            4,372\n",
      "_             1,035\n",
      "core          4,563\n",
      "_             1,035\n",
      "web           4,773\n",
      "_             1,035\n",
      "sm           15,488\n",
      "=             1,027\n",
      "now           2,085\n",
      "##pass       15,194\n",
      "##ing         2,075\n",
      "it            2,009\n",
      "to            2,000\n",
      "spa          12,403\n",
      "##cy          5,666\n",
      "and           1,998\n",
      "loading      10,578\n",
      "the           1,996\n",
      "default      12,398\n",
      "english       2,394\n",
      "model         2,944\n",
      "in            1,999\n",
      "spa          12,403\n",
      "##cy          5,666\n",
      "as            2,004\n",
      "nl           17,953\n",
      "##p           2,361\n",
      "the           1,996\n",
      "display       4,653\n",
      "cm            4,642\n",
      "##d           2,094\n",
      "gives         3,957\n",
      "us            2,149\n",
      "the           1,996\n",
      "text          3,793\n",
      "from          2,013\n",
      "the           1,996\n",
      "pdf          11,135\n",
      "with          2,007\n",
      "marked        4,417\n",
      "criteria      9,181\n",
      "##s           2,015\n",
      "which         2,029\n",
      "are           2,024\n",
      "important     2,590\n",
      "and           1,998\n",
      "relevant      7,882\n",
      "like          2,066\n",
      "name          2,171\n",
      ",             1,010\n",
      "date          3,058\n",
      ",             1,010\n",
      "geo          20,248\n",
      "##pol        18,155\n",
      "##itical     26,116\n",
      "entity        9,178\n",
      ",             1,010\n",
      "and           1,998\n",
      "org           8,917\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
    "# tokenizer's behavior, let's also get the token strings and display them.\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# For each token and its id...\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "uJ7PRx2dKqFN"
   },
   "outputs": [],
   "source": [
    "# Search the input_ids for the first instance of the `[SEP]` token.\n",
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "# The number of segment A tokens includes the [SEP] token istelf.\n",
    "num_seg_a = sep_index + 1\n",
    "\n",
    "# The remainder are segment B.\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "# Construct the list of 0s and 1s.\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "# There should be a segment_id for every input token.\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a30sBTcqQv6X"
   },
   "source": [
    ">*Side Note: Where's the padding?*\n",
    ">\n",
    "> The original [example code](https://huggingface.co/transformers/model_doc/bert.html?highlight=bertforquestionanswering#transformers.BertForQuestionAnswering) does not perform any padding. I suspect that this is because we are only feeding in a *single example*. If we instead fed in a batch of examples, then we would need to pad or truncate all of the samples in the batch to a single length, and supply an attention mask to tell BERT to ignore the padding tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNwhEw0kQPBN"
   },
   "source": [
    "We're ready to feed our example into the model!\n",
    "\n",
    "Note: The result object is of type [QuestionAnsweringModelOutput](https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.QuestionAnsweringModelOutput)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "DQiKr6Aw-YTg"
   },
   "outputs": [],
   "source": [
    "# Run our example through the model.\n",
    "outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                             token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
    "                             return_dict=True) \n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBdS_QkIbDzh"
   },
   "source": [
    "Now we can highlight the answer just by looking at the most probable start and end words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LeUQ44hAJmn9",
    "outputId": "a060a6de-b02e-4aac-951b-cf4f29aa741d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"convert _ from _ path\"\n"
     ]
    }
   ],
   "source": [
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twMUWmr2brRw"
   },
   "source": [
    "It got it right! Awesome :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cERYCGKMbOXX"
   },
   "source": [
    "> *Side Note: It's a little naive to pick the highest scores for start and end--what if it predicts an end word that's before the start word?! The correct implementation is to pick the highest total score for which end >= start.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6j2znkwXYsn"
   },
   "source": [
    "With a little more effort, we can reconstruct any words that got broken down into subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Khral6HZXCuI",
    "outputId": "10fd61d4-a74c-4015-daf9-38d5087cf3b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"gv\"\n"
     ]
    }
   ],
   "source": [
    "# Start with the first token.\n",
    "answer = tokens[answer_start]\n",
    "\n",
    "# Select the remaining answer tokens and join them with whitespace.\n",
    "for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "    # If it's a subword token, then recombine it with the previous token.\n",
    "    if tokens[i][0:2] == '##':\n",
    "        answer += tokens[i][2:]\n",
    "    \n",
    "    # Otherwise, add a space then the token.\n",
    "    else:\n",
    "        answer += ' ' + tokens[i]\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hh6nkIdXq-O"
   },
   "source": [
    "## 4. Visualizing Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hG2YCHYXtg-"
   },
   "source": [
    "I was curious to see what the scores were for all of the words. The following cells generate bar plots showing the start and end scores for every word in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gkKFa73eJkPE"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Use plot styling from seaborn.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarkgrid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "#sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7kazkb2iEuQ"
   },
   "source": [
    "Retrieve all of the start and end scores, and use all of the tokens as x-axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C56AtMg2UBxN"
   },
   "outputs": [],
   "source": [
    "# Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays.\n",
    "s_scores = start_scores.detach().numpy().flatten()\n",
    "e_scores = end_scores.detach().numpy().flatten()\n",
    "\n",
    "# We'll use the tokens as the x-axis labels. In order to do that, they all need\n",
    "# to be unique, so we'll add the token index to the end of each one.\n",
    "token_labels = []\n",
    "for (i, token) in enumerate(tokens):\n",
    "    token_labels.append('{:} - {:>2}'.format(token, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIaW7RyTiLeu"
   },
   "source": [
    "Create a bar plot showing the score for every input word being the \"start\" word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "y6OAV1dL3-UB",
    "outputId": "4a0e01b4-65ea-445f-db89-6f5d11359957"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a barplot showing the start word score for all of the tokens.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mbarplot(x\u001b[38;5;241m=\u001b[39mtoken_labels, y\u001b[38;5;241m=\u001b[39ms_scores, ci\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Turn the xlabels vertical.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xticklabels(ax\u001b[38;5;241m.\u001b[39mget_xticklabels(), rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m, ha\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a barplot showing the start word score for all of the tokens.\n",
    "ax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n",
    "\n",
    "# Turn the xlabels vertical.\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "# Turn on the vertical grid to help align words to scores.\n",
    "ax.grid(True)\n",
    "\n",
    "plt.title('Start Word Scores')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIwrF7y6iS1l"
   },
   "source": [
    "Create a second bar plot showing the score for every input word being the \"end\" word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "6tXEqIp-Tzou",
    "outputId": "33ec2d98-af47-4f74-ae04-5e78da75ee55"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a barplot showing the end word score for all of the tokens.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mbarplot(x\u001b[38;5;241m=\u001b[39mtoken_labels, y\u001b[38;5;241m=\u001b[39me_scores, ci\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Turn the xlabels vertical.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xticklabels(ax\u001b[38;5;241m.\u001b[39mget_xticklabels(), rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m, ha\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a barplot showing the end word score for all of the tokens.\n",
    "ax = sns.barplot(x=token_labels, y=e_scores, ci=None)\n",
    "\n",
    "# Turn the xlabels vertical.\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "# Turn on the vertical grid to help align words to scores.\n",
    "ax.grid(True)\n",
    "\n",
    "plt.title('End Word Scores')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awgi7Z_a9KSq"
   },
   "source": [
    "**Alternate View**\n",
    "\n",
    "I also tried visualizing both the start and end scores on a single bar plot, but I think it may actually be more confusing then seeing them separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "m4VUk6R05uXS"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Store the tokens and scores in a DataFrame. \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Each token will have two rows, one for its start score and one for its end\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# score. The \"marker\" column will differentiate them. A little wacky, I know.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Store the tokens and scores in a DataFrame. \n",
    "# Each token will have two rows, one for its start score and one for its end\n",
    "# score. The \"marker\" column will differentiate them. A little wacky, I know.\n",
    "scores = []\n",
    "for (i, token_label) in enumerate(token_labels):\n",
    "\n",
    "    # Add the token's start score as one row.\n",
    "    scores.append({'token_label': token_label, \n",
    "                   'score': s_scores[i],\n",
    "                   'marker': 'start'})\n",
    "    \n",
    "    # Add  the token's end score as another row.\n",
    "    scores.append({'token_label': token_label, \n",
    "                   'score': e_scores[i],\n",
    "                   'marker': 'end'})\n",
    "    \n",
    "df = pd.DataFrame(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "07xyo-I97Ntt",
    "outputId": "2916150b-0017-485f-81c4-bde4c87d3faf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Draw a grouped barplot to show start and end scores for each word.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# The \"hue\" parameter is where we tell it which datapoints belong to which\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# of the two series.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mcatplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_label\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarker\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m      5\u001b[0m                 kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m\"\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Turn the xlabels vertical.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m g\u001b[38;5;241m.\u001b[39mset_xticklabels(g\u001b[38;5;241m.\u001b[39max\u001b[38;5;241m.\u001b[39mget_xticklabels(), rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m, ha\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# Draw a grouped barplot to show start and end scores for each word.\n",
    "# The \"hue\" parameter is where we tell it which datapoints belong to which\n",
    "# of the two series.\n",
    "g = sns.catplot(x=\"token_label\", y=\"score\", hue=\"marker\", data=df,\n",
    "                kind=\"bar\", height=6, aspect=4)\n",
    "\n",
    "# Turn the xlabels vertical.\n",
    "g.set_xticklabels(g.ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "# Turn on the vertical grid to help align words to scores.\n",
    "g.ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UyBYNmeegGf"
   },
   "source": [
    "## 5. More Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWtcRpPef-Ce"
   },
   "source": [
    "Turn the QA process into a function so we can easily try out other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rH8NbBlsfxZ_"
   },
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example through the model.\n",
    "    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
    "                    return_dict=True) \n",
    "\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVlKTK-njWrX"
   },
   "source": [
    "As our reference text, I've taken the Abstract of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4VPq6FdjxyX",
    "outputId": "c58d2447-6493-4dc1-f717-4f66a707d72a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We introduce a new language representation model called BERT, which stands for\n",
      "Bidirectional Encoder Representations from Transformers. Unlike recent language\n",
      "representation models (Peters et al., 2018a; Radford et al., 2018), BERT is\n",
      "designed to pretrain deep bidirectional representations from unlabeled text by\n",
      "jointly conditioning on both left and right context in all layers. As a result,\n",
      "the pre-trained BERT model can be finetuned with just one additional output\n",
      "layer to create state-of-the-art models for a wide range of tasks, such as\n",
      "question answering and language inference, without substantial taskspecific\n",
      "architecture modifications. BERT is conceptually simple and empirically\n",
      "powerful. It obtains new state-of-the-art results on eleven natural language\n",
      "processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute\n",
      "improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1\n",
      "question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD\n",
      "v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "bert_abstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
    "\n",
    "print(wrapper.fill(bert_abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEB654YCknYv"
   },
   "source": [
    "-----------------------------\n",
    "Ask BERT what its name stands for (the answer is in the first sentence of the abstract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfntqRCBegGj",
    "outputId": "b92fae04-b47c-4d41-ab6a-534fee04aa8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 258 tokens.\n",
      "\n",
      "Answer: \"bidirectional encoder representations from transformers\"\n"
     ]
    }
   ],
   "source": [
    "question = \"What does the 'B' in BERT stand for?\"\n",
    "\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6HcijzxkTO9"
   },
   "source": [
    "---------------------\n",
    "Ask BERT about example applications of itself :)\n",
    "\n",
    "The answer to the question comes from this passage from the abstract: \n",
    "\n",
    "> \"...BERT model can be finetuned with just one additional output\n",
    "layer to create state-of-the-art models for **a wide range of tasks, such as\n",
    "question answering and language inference,** without substantial taskspecific\n",
    "architecture modifications.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVNVGN5-gI06",
    "outputId": "db986fa1-3237-4db4-de8e-3bf564bfe176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query has 255 tokens.\n",
      "\n",
      "Answer: \"question answering and language inference\"\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some example applications of BERT?\"\n",
    "\n",
    "answer_question(question, bert_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Question Answering with a Fine-Tuned BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0325dd797ca640cea44dc5d2d50d3378": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bdec4b4708b1446bbeb00ddef16511c7",
       "IPY_MODEL_183df2bf188a4f9fba59234e57b205e4"
      ],
      "layout": "IPY_MODEL_a5d823c6c36f4ffaa54de63b7df27187"
     }
    },
    "0f4a53ffb84641be966022b5f995d597": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15cf214d3f95421790d246a7fdc8d200": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17108037891642fd8535f898ffc1c666": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2e1db44876aa4bda8f432424d43ef49d",
       "IPY_MODEL_d2057bb0a03148d89ff752025b209034"
      ],
      "layout": "IPY_MODEL_454c54b5f7e34caabfcdfb3751647f1d"
     }
    },
    "183df2bf188a4f9fba59234e57b205e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba7cd4be6dcb457db7197cd326a56793",
      "placeholder": "",
      "style": "IPY_MODEL_3f1e67a35637482e9a2db7e7233d02c2",
      "value": " 1.34G/1.34G [00:22&lt;00:00, 58.7MB/s]"
     }
    },
    "296fc3b5f35f49a4a8b71ba7450f39ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c9c2fd7eece41589d73d9a238225905": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e1db44876aa4bda8f432424d43ef49d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_658e95de041f4b6abbd70a8071253297",
      "max": 443,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_625f0bb8d61443ffa9aec2e2f52db015",
      "value": 443
     }
    },
    "3f1e67a35637482e9a2db7e7233d02c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "454c54b5f7e34caabfcdfb3751647f1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54decd45844c47ea8c2816ef729e7445": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "604df6fab59c46438be9ac909632e6fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9c50285033b941c19bb0d6cfefa35a92",
       "IPY_MODEL_9cdafc71790e4a65afdea6267db7ea29"
      ],
      "layout": "IPY_MODEL_54decd45844c47ea8c2816ef729e7445"
     }
    },
    "625f0bb8d61443ffa9aec2e2f52db015": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "658e95de041f4b6abbd70a8071253297": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7773da342fd34ddd8d98d22308ff84a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9c50285033b941c19bb0d6cfefa35a92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15cf214d3f95421790d246a7fdc8d200",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7773da342fd34ddd8d98d22308ff84a0",
      "value": 231508
     }
    },
    "9cdafc71790e4a65afdea6267db7ea29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7052a47457d44e2b4914734294eb671",
      "placeholder": "",
      "style": "IPY_MODEL_2c9c2fd7eece41589d73d9a238225905",
      "value": " 232k/232k [00:00&lt;00:00, 739kB/s]"
     }
    },
    "a5d823c6c36f4ffaa54de63b7df27187": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b871a4177dba4977813c16a6a71d3b27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba7cd4be6dcb457db7197cd326a56793": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdec4b4708b1446bbeb00ddef16511c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b871a4177dba4977813c16a6a71d3b27",
      "max": 1340675298,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eb910005a7ae4e04ab28ac8d506e9f9c",
      "value": 1340675298
     }
    },
    "c7052a47457d44e2b4914734294eb671": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2057bb0a03148d89ff752025b209034": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f4a53ffb84641be966022b5f995d597",
      "placeholder": "",
      "style": "IPY_MODEL_296fc3b5f35f49a4a8b71ba7450f39ba",
      "value": " 443/443 [00:00&lt;00:00, 919B/s]"
     }
    },
    "eb910005a7ae4e04ab28ac8d506e9f9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
